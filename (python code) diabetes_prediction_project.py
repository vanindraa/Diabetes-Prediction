# -*- coding: utf-8 -*-
"""Diabetes Prediction Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UbfBbA8ZrgEFft1woiBzX6Fg8uJFSKez

# DATA DESC
The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor vari- ables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

- Pregnancies: Number of times pregnant
- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
    - <70 = hypoglicemic
    - 70 - 100 = normal
    - 100 - 126 = pre-diabetic
    - 126 - 180 = diabetic
    - .> 180 = hyperglycemic
    - Taken from https://dtc.ucsf.edu/types-of-diabetes/type2/understanding-type-2-diabetes/basic-facts/diagnosing-diabetes/#oral
- BloodPressure: Diastolic blood pressure (mm Hg)
    - <90 = Low
    - 90-119 = Normal
    - 120 - 129 = Elevated
    - 130 - 139 = High Stage 1
    - 140 - 180 = High stage 2
    - .>180 = Hypertensive
    - Source: https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings
- SkinThickness: Triceps skin fold thickness (mm)
- Insulin: 2-Hour serum insulin (mu U/ml)
- BMI: Body mass index (weight in kg/(height in m)2)
    - < 18.5 = Underweight
    - 18.5 - 24.9 = Healthy
    - 25 - 29.9 = Overweight
    - .>=30 = Obese
        - 30 - 34.9 = Obese Class 1
        - 35 - 39.9 = Obese Class 2
        - .>= 40 = Obese Class 3 (Severe Obesity)
    - Source: https://www.cdc.gov/obesity/basics/adult-defining.html
- DiabetesPedigreeFunction: Diabetes pedigree function -> the likelihood of someone having diabetes (probably in %)
- Age:Age(years)
- Outcome: Classvariable(0 or 1)

# MAIN GOAL
To create a Machine Learning model that can predict/detect if someone is diabetic or not.

# 1. IMPORTING  PACKAGES AND DATASET
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import pylab as py
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler

import plotly 
import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as ply
from plotly.offline import iplot
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

data = pd.read_csv('https://raw.githubusercontent.com/vanindraa/vanindrarepo/Dataset/Dataset9_Diabetes_Prediction.csv')
data.head()

#Simplifying feature's name
data.rename(columns={'DiabetesPedigreeFunction' : 'Diabetes_Chance'}, inplace=True)

plt.rcParams["figure.figsize"] = [10, 10]
plt.rcParams["figure.autolayout"] = True

#Distribusi data raw
data.hist()
plt.show()

data.shape

"""dataset diabetes prediction memiliki 768 rows dan 9 columns"""

data.info()

data.isnull().sum()

"""No null so it is good, but let's check the complete descrtiption

### Profiling Descriptive Stats
"""

data.describe()

"""Key takeaway: In this data, we are going to focus on the feature 'Outcome' as it is what we are trying to predict and it is the target for this dataset. For the model itself, because we are classifying who is diabetes and who is not, we are going to use the classification model and not regression. As for the info from the table above, not much can be extracted from the feature'Outcome'. However, I am sure there are some nonsensical values: Glucose, BloodPressure, SkinThickness, and BMI can't reach 0. Therefore, we treat them as missing value.

#### Replacing missing values
"""

data['BMI']=data['BMI'].replace([0],data['BMI'].median())
data['Glucose']=data['Glucose'].replace([0],data['Glucose'].median())
data['BloodPressure']=data['BloodPressure'].replace([0],data['BloodPressure'].median())
data['SkinThickness']=data['SkinThickness'].replace([0],data['SkinThickness'].median())
data['Insulin']=data['Insulin'].replace([0],data['SkinThickness'].median())
data.describe()

#Distribusi data setelah missing value (0) diisi dengan median
data.hist()
plt.show()

"""# 2. DATA PREPROCESSING

## Pregnancies Feature
"""

#QQ-Plot
stats.probplot(data['Pregnancies'], dist='norm',plot=py)
py.show()

"""Key takeaway, this is not even close to being standard.

### Finding Outliers
"""

plt.boxplot(data['Pregnancies'])
plt.show()

Q1 = np.percentile(data.Pregnancies,25)
Q3 = np.percentile(data.Pregnancies,75)
IQR = (Q3-Q1)
print("IQR sama dengan", IQR)

Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['Pregnancies']<Lbound)|(data['Pregnancies']>Ubound)]

"""Key takeaway: Outliers untuk fitur pregnancies berada pada value di bawah -6.5 dan di atas 13.5.Terdapat empat row yang berada pada range itu dan merupakan outlier. Solusinya, di drop.

## Glucose Feature
"""

#QQ-Plot
stats.probplot(data['Glucose'], dist='norm',plot=py)
py.show()

"""Key Takeaway: Berdasarkan QQplot, dapat dilihat bahwa fitur Glocose mendekati persebaran normal

### Finding Outliers
"""

plt.boxplot(data['Glucose'])
plt.show()

Q1 = np.percentile(data.Glucose,25)
Q3 = np.percentile(data.Glucose,75)
IQR = (Q3-Q1)
print("IQR sama dengan", IQR)

Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['Glucose']<Lbound)|(data['Glucose']>Ubound)]

"""Key takeaway: Tidak ada outliers untuk fitur Glucose

## BloodPressure Feature
"""

#QQ-Plot
stats.probplot(data['BloodPressure'], dist='norm',plot=py)
py.show()

"""Key Takeaway: Titik-titik pada QQplot diatas menunjukkan jika persebaran hampir normal untuk fitur BloodPressure.

### Finding Outliers
"""

plt.boxplot(data['BloodPressure'])
plt.show()

Q1 = np.percentile(data.BloodPressure,25)
Q3 = np.percentile(data.BloodPressure,75)
IQR = (Q3-Q1)
Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print("IQR sama dengan", IQR)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['BloodPressure']<Lbound)|(data['BloodPressure']>Ubound)]

#jumlah row outlier BloodPressure
outlier_outlier1 = data.loc[(data['BloodPressure']<Lbound)|(data['BloodPressure']>Ubound)]
outlier_outlier1['BloodPressure'].count()

"""Key Takeaway: Outliers untuk fitur BloodPressure berada pada value di bawah 40 dan di atas 104. Jumlah outliers pada fitur ini adalah 14 buah

## Skin Thickness Feature
"""

#QQ-Plot
stats.probplot(data['SkinThickness'], dist='norm',plot=py)
py.show()

"""Key Takeaway: Berdasarkan titik-titik di visualisasi tersebut, sebaran fitur Skin Thickness tidak menunjukkan sebaran normal

### Finding Outliers
"""

plt.boxplot(data['SkinThickness'])
plt.show()

Q1 = np.percentile(data.SkinThickness,25)
Q3 = np.percentile(data.SkinThickness,75)
IQR = (Q3-Q1)
Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print("IQR sama dengan", IQR)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['SkinThickness']<Lbound)|(data['SkinThickness']>Ubound)]

#jumlah row outlier SkinThickness
outlier_outlier2 = data.loc[(data['SkinThickness']<Lbound)|(data['SkinThickness']>Ubound)]
outlier_outlier2['SkinThickness'].count()

"""Key Takeaway: Outliers pada fitur Skin Thickness berada pada value di bawah 3.3 dan di atas 49.1. Jumlah outliers pada fitur ini adalah 35 buah

## Insulin Feature
"""

#QQ-Plot
stats.probplot(data['Insulin'], dist='norm',plot=py)
py.show()

"""Key takeaway: Berdasarkan QQplot, terlihat bahwa titik-titik  di atas menjauhi garis tengah sehingga dapat disimpulkan jika persebarannya tidak merata. Selain itu, fitur insulin juga terdapat outliers

### Finding Outliers
"""

plt.boxplot(data['Insulin'])
plt.show()

Q1 = np.percentile(data.Insulin,25)
Q3 = np.percentile(data.Insulin,75)
IQR = (Q3-Q1)
Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print("IQR sama dengan", IQR)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['Insulin']<Lbound)|(data['Insulin']>Ubound)]

#jumlah row outlier Insulin
outlier_outlier3 = data.loc[(data['Insulin']<Lbound)|(data['Insulin']>Ubound)]
outlier_outlier3['Insulin'].count()

"""Key Takeaway: Outlier pada fitur Insulin berada pada value di bawah -122.5 dan di atas value 265.5. DIdasarkan pada boxplot, outlier hanya ada di area yang melebihi upperbound. Jumlah outliers pada fitur ini adalah 44 buah.

## BMI Feature
"""

#QQ-Plot
stats.probplot(data['BMI'], dist='norm',plot=py)
py.show()

"""Key Takeaway: Berdasarkan QQplot, terlihat bahwa titik-titik rata-rata menempel dan mendekati garis tengah sehingga menunjukkan bahwa sebaran BMI ada yang menyebar secara normal.

### Finding Outliers
"""

plt.boxplot(data['BMI'])
plt.show()

Q1 = np.percentile(data.BMI,25)
Q3 = np.percentile(data.BMI,75)
IQR = (Q3-Q1)
Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print("IQR sama dengan", IQR)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['BMI']<Lbound)|(data['BMI']>Ubound)]

#jumlah row outlier BMI
outlier_outlier4 = data.loc[(data['BMI']<Lbound)|(data['BMI']>Ubound)]
outlier_outlier4['BMI'].count()

"""Key Takeaway: Outlier pada fitur BMI berada pada value di bawah 14.5 dan di atas value 49. DIdasarkan pada boxplot, outlier hanya ada di area yang melebihi upperbound. Jumlah outliers pada fitur ini adalah 8 buah

## All the histogram
"""

#All Histogram
data.hist()
plt.show()

"""Key Takeaway: Tidak ada data yang memiliki persebaran normal sempurna. Fitur yang mendekati persebaran normal adalah Glucose, BMI, dan Blood Pressure. Fitur Diabetes Chance, Age, Insulin, dan Pregnancies persebarannya skew ke kanan.

## Pair Plot
"""

#Data Pairplot
sns.pairplot(data)

"""## CatPlot"""

sns.catplot(data=data, height=5, aspect=2)

"""## Data Shape after finding outliers """

data.shape

"""### Duplicated values"""

data[data.duplicated()] #There are no duplicated values

"""## Imbalanced Data"""

# Ketimpangan data
data.groupby('Outcome').size()

data.Outcome.value_counts(normalize=True)

"""Key Takeaaway: Data yang diberikan timpang antara outcome 0 dan 1. Jumlah yang tidak terkena diabetes hampir 2x jumlah yang terkena diabetes. Ini menunjukkan perlunya balancing data dengan oversampling atau undersampling. Namun, secara persentase, outcome 0 memiliki proporsi 31% dari total data sehingga dapat dikatakan data ini mildly imbalanced. (sumber: https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data)

# 3. EDA

## Correlation Between Features
"""

corr = data.corr()
corr

sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,
            annot=True)

"""Key Takeaway: Fitur akan dianggap berkorelasi saat koefisiennya >0.5. Jika koefisien berada <0.5, maka fitur tidak dianggap berkorelasi. Untuk fitur outcome sendiri, tidak berkorelasi dengan fitur manapun. Paling dekat adalah fitur 'Glucose' dengan koefisien 0,49 sehingga mungkin dapat dianggap sedikit berkorelasi.

Jika dilihat dari heat map, fitur yang berkaitan adalah BMI dengan Skin Thickness dengan koefisien 0.53, dan fitur Age dengan Pregnancies dengan koefisien 0.54

### Glucose and BMI
We are trying to see if BMI and Glucose actually gives out a good correlation
"""

plt.scatter(data['Glucose'],data['BMI'])
plt.show()

#Relational plot
sns.relplot(x='Glucose',y='BMI', hue='Outcome',data=data)

sns.jointplot(x='Glucose', y='BMI', data=data, kind="reg")

"""Key takeaway: Glucose dan BMI terlihat memberikan korelasi yang positif jika didasarkan pada garis di atas. Semakin besar glucose (gula darah), maka BMI akan bertambah pula.

### Pregnancies vs BMI
"""

plt.scatter(data['Pregnancies'],data['BMI'])
plt.show()

#Relational plot
sns.relplot(x='Pregnancies',y='BMI', hue='Outcome',data=data)

sns.jointplot(x='Pregnancies', y='BMI', data=data, kind="reg")

"""Key takeaway: Garis di atas terlihat rata sehingga dapat disimpulkan jika tidak ada hubungan positif maupun negatif antara jumlah kehamilan dengan angka BMI

### Pregnancies vs Age
"""

plt.scatter(data['Pregnancies'],data['Age'])
plt.show()

#Relational plot
sns.relplot(x='Pregnancies',y='Age', hue='Outcome',data=data)

sns.jointplot(x='Pregnancies', y='Age', data=data, kind="reg")

"""Key Takeaway: Berdasarkan heatmap juga plot di atas, jumlah kehamilan dan umur memiliki hubungan yang positif. Semakin besar jumlah kehamilan, maka umur juga semakin tua. In a sense ini masuk akal, namun tidak ada korelasi nyata terhadap jumlah kehamilan dan umur.

### Skin Thickness vs BMI
"""

plt.scatter(data['SkinThickness'],data['BMI'])
plt.show()

#Relational plot
sns.relplot(x='SkinThickness',y='BMI', hue='Outcome',data=data)

sns.jointplot(x='SkinThickness', y='BMI', data=data, kind="reg")

"""Key Takeaway: Ketebalan Kulit memiliki hubungan yang positif dengan tingkat BMI. semakin tebal kulit, maka semakin tinggi pula BMInya

## Kesimpulan EDA 1
Dapat disimpulkan jika variabel yang tersedia berhubungan lemah satu antar satu dengan lainnya. Variabel yang berhubungan hanya Umur dengan Jumlah kehamilan dan Ketebalan kulit dengan BMI. Untuk Outcome sendiri, memiliki hubungan yang cukup dengan variabel Glucose.
#### 
Walaupun demikian, terdapat beberapa fitur yang seharusnya dapat digunakan untuk memprediksi seseorang terkena diabetes atau tidak. Fitur-fitur tersebut adalah:
- Glucose -> Gula Darah
- BloodPressure -> Tekanan Darah
- Skin Thickness -> Ketebalan Kulit
- Insulin
- BMI
- Diabetes_Chance -> Chance seseorang terkena diabetes berdasarkan histori keluarga

# 4. EDA 2 -> FEATURES VS OUTCOME
"""

data.describe()

data.corr()

"""## Pregnancies vs Outcome"""

sns.jointplot(x='Pregnancies', y='Outcome', data=data, kind="reg")

"""Key takeaway: Angka kehamilan memiliki hubungan yang positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.2"""

#explore pregnancies vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['Pregnancies'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['Pregnancies'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('Pregnancies')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang positif diabetes cenderung memiliki
riwayat kehamilan (jumlah kehamilan) yang cukup
tinggi (> 20 Kali). Pasien diabetes paling banyak
pada jumlah kehamilan 2 kali`

## Glucose vs Outcome
"""

sns.jointplot(x='Glucose', y='Outcome', data=data, kind="reg")

"""Key takeaway: Tingkat gula darah memiliki korelasi positif yang cukup kuat dengan hasil tes (outcome) dengan koefisien 0.48"""

#explore glucose vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['Glucose'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['Glucose'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('Glucose')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang terkena diabetes memiliki nilai glucose yang
lebih tinggi daripada orang-orang yang non-diabetes.
Penderita diabetes paling banyak pada nilai glucose > 125

## Blood Pressure vs Outcome
"""

sns.jointplot(x='BloodPressure', y='Outcome', data=data, kind="reg")

"""Key takeaway: Tekanan darah memiliki korelasi positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.16"""

#explore bloodpressure vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['BloodPressure'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['BloodPressure'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('BloodPressure')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang terkena diabetes memiliki nilai bloodPressure
yang lebih tinggi daripada orang-orang yang nondiabetes.
Terbanyak diderita oleh pasien dengan
bloodPressure >75an.

## Skin Thickness vs Outcome
"""

sns.jointplot(x='SkinThickness', y='Outcome', data=data, kind="reg")

"""Key takeaway: Ketebalan kulit memiliki korelasi positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.17"""

#explore skinthickness vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['SkinThickness'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['SkinThickness'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('SkinThickness')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang positif diabetes cenderung memiliki ketebalan
kulit (skinthickness) yang cukup tinggi (hingga nilai 50) .
Range skinThickness diabetes 25-40an dan paling banyak
dititik 25an.

## Insulin vs Outcome
"""

sns.jointplot(x='Insulin', y='Outcome', data=data, kind="reg")

"""Key takeaway: Injeksi insulin memiliki korelasi positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.15"""

#explore Insulin vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['Insulin'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['Insulin'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('Insulin')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang terkena diabetes memiliki insulin berada pada
range 0-600an dan paling banyak diderita oleh orang
dengan range insulin 0-170an.

## BMI vs Outcome
"""

sns.jointplot(x='BMI', y='Outcome', data=data, kind="reg")

"""Key takeaway: Body Mass Index (BMI) memiliki korelasi positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.29"""

#explore bmi vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['BMI'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['BMI'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('BMI')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang terkena diabetes memiliki nilai BMI pada range
20-70 an dan paling banyak diderita oleh pasien dengan BMI
pada range 30-40 artinya sudah masuk kategori
overweight/obese

## Diabetes Chance vs Outcome
"""

sns.jointplot(x='Diabetes_Chance', y='Outcome', data=data, kind="reg")

"""Key takeaway: Riwayat histori diabetes keluarga memiliki korelasi positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.18"""

#explore diabetesChance vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['Diabetes_Chance'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['Diabetes_Chance'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('Diabetes_Chance')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang positif diabetes memiliki nilai diabetesChance
pada kisaran 0-2.5. Paling banyak diderita oleh pasien
dengan nilai DiabetesChance pada range 0-0.5. Semakin
tinggi nilai diabetesChance maka semakin besar peluang
orang tersebut positif diabetes.

## Age vs Outcome
"""

sns.jointplot(x='Age', y='Outcome', data=data, kind="reg")

"""Key takeaway: Umur memiliki korelasi positif dengan hasil tes (outcome) walaupun hubungannya lemah dengan koefisien 0.23"""

#explore age vs outcome

plt.figure(figsize=(13,6))
g = sns.kdeplot(data['Age'][data['Outcome'] == 1], color='Red', shade=True)
g = sns.kdeplot(data['Age'][data['Outcome'] == 0], ax= g, color='Green', shade=True)
g.set_xlabel('Age')
g.set_ylabel('Frequency')
g.legend(['Positive', 'Negative'])

"""orang yang terkena diabetes terbanyak berada pada
rentang usia 20-50 tahun.

# 5. Feature Engineering
Adding some features and after that we will do a second wave of EDA
"""

data.head()

#Features adding function
def bmi_status(row):
    if row['BMI']<18.5:
        return 'underweight'
    elif row ['BMI'] == 18.5 and row['BMI'] <25:
        return 'healthy'
    elif row['BMI'] == 25 and row['BMI']<30:
        return 'overweight'
    else:
        return 'obese'

def blood_press(row):
    if row['BloodPressure'] <90:
        return 'Low'
    elif row['BloodPressure'] == 90 and row['BloodPressure'] <120:
        return 'Normal'
    elif row['BloodPressure'] == 120 and row['BloodPressure'] <130:
        return 'Elevated'
    elif row['BloodPressure'] == 130 and row['BloodPressure'] <140:
        return 'High Stage 1'
    elif row['BloodPressure'] == 140 and row['BloodPressure'] <180:
        return 'High Stage 2'
    else:
        return 'Hypertensive'
    
def glucose_level(row):
    if row['Glucose'] == 70 and row['Glucose'] < 100:
        return 'Normal'
    elif row['Glucose'] == 100 and row['Glucose'] <126:
        return 'Symptomatic'
    elif row['Glucose'] <70:
        return 'Hypoglicemic'
    elif row['Glucose'] == 126 and row['Glucose']<=180:
        return 'Diabetic'
    else:
        return 'Hyperglycemic'

def test_result(row):
    if row['Outcome']==1:
        return 'Positive'
    else:
        return 'Negative'
    
def is_obese(row):
    if row['BMI']>=30:
        return 'Yes'
    else:
        return 'No'

def is_diabetic(row): #ini kayanya gaperlu, udah ada outcome soalnya
    if row['Outcome']==1:
        return 'Yes'
    else:
        return 'No'
    
data['BMI_status']=data.apply(bmi_status,axis=1)
data['Pressure_status']=data.apply(blood_press, axis=1)
data['Diabetes_status']=data.apply(glucose_level, axis=1)
data['test_result']=data.apply(test_result, axis=1)
data['is_obese']=data.apply(is_obese,axis=1)
data['is_diabetic']=data.apply(is_diabetic,axis=1)
data.head()

data.info()

print(data['BMI_status'].unique(), '<- Unique BMI category')

print(data['Pressure_status'].unique(),'<- Unique Blood Pressure status')

print(data['Diabetes_status'].unique(),'<- Unique Diabetes Category')

"""Key Takeaway: Terdapat empat buah fitur kategorical yang ditambahkan:
- test_result -> Hasil tes diabetes
    - Positive
    - Negative
- BMI_status -> Kategori BMI 
    - Underweight
    - Obese
    - Overweight
- Pressure_status -> Kategori tekanan darah
    - Low
    - Normal
    - Hypertensive
- Diabetes_status -> Kategori tingkat gula darah
    - Diabetic
    - Hypoglicemic
    - Symptomatic
    - Hyperglycemic

# 6. Categorical EDA -> Categorical Insight

## Outcome
- 0 = not diabetic/Negative
- 1 = tested as diabetic/Positive
"""

outcome = data.groupby('test_result')[['Outcome']].count().reset_index()
outcome

fig = px.bar(outcome, x='test_result', y= 'Outcome', color='test_result', )
fig.update_layout(xaxis={'categoryorder':'total ascending'})
fig.show()

"""Key Takeaway: Dari data yang ada, dapat disimpulkan jika 500
orang negatif diabetes dan 268 orang lainnya
positif diabetes.

## Diabetes_Status
"""

Diabetic = data.groupby('Diabetes_status')[['Outcome']].count().reset_index()
Diabetic

fig = px.bar(Diabetic, x='Diabetes_status', y= 'Outcome', color='Diabetes_status', )
fig.update_layout(xaxis={'categoryorder':'total descending'})
fig.show()

"""Key Takeaway:Dari data yang diberikan, jumlah orang yang tergolong Diabetic (gula darah tinggi) adalah 9 orang, orang yang symptomatic (memiliki tingkat gula darah lebih tinggi dari normal) berjumlah 17 orang, dan 11 orang terindikasi Hypoglicemic (Gula darah di bawah normal). 731 Orang lainnya terindikasi Hyperglycemic (gula darah ekstrim).

## BMI Status
"""

bmi = data.groupby('BMI_status')[['Outcome']].count().reset_index()
bmi

fig = px.bar(bmi, x='BMI_status', y= 'Outcome', color='BMI_status', )
fig.update_layout(xaxis={'categoryorder':'total descending'})
fig.show()

"""Key Takeaway: Dari data yang diberikan, sangat didominasi oleh orang
yang obesitas yakni 758 orang. Sangat tidak imbang
terhadap status overweight (6 orang) dan underweight
(4 orang)

## Blood Pressure Level
"""

pressure = data.groupby('Pressure_status')[['Outcome']].count().reset_index()
pressure

fig = px.bar(pressure, x='Pressure_status', y= 'Outcome', color='Pressure_status', )
fig.update_layout(xaxis={'categoryorder':'total descending'})
fig.show()

"""Key Takeaway: Melalui data yang diberikan, mayoritas adalah mereka
yang memiliki tekanan darah rendah sebanyak 708
orang. Ada 38 orang yang terindikasi hipertensi dan 22
orang yang memiliki tekanan darah normal.

# 8. Feature Scalling
"""

data.hist()
plt.show()

#Importing MinMaxScaler to do scalling
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler

#Compare feature scaling methods

scaled_compare = data.copy()

#standard scaler
ss = StandardScaler()
scaled_compare['Glucose_standard_scaler']= ss.fit_transform(scaled_compare[['Glucose']])
scaled_compare['Diabeteschance_standard_scaler']= ss.fit_transform(scaled_compare[['Diabetes_Chance']])

#robust scaler
rs = RobustScaler()
scaled_compare['Glucose_robust_scaler']= rs.fit_transform(scaled_compare[['Glucose']])
scaled_compare['Diabeteschance_robust_scaler']= rs.fit_transform(scaled_compare[['Diabetes_Chance']])

#minmax scaler
mms = MinMaxScaler()
scaled_compare['Glucose_minmax_scaler']= mms.fit_transform(scaled_compare[['Glucose']])
scaled_compare['Diabeteschance_minmax_scaler']= mms.fit_transform(scaled_compare[['Diabetes_Chance']])


scaled_compare.head()

# #Defining the scaler
minscaler = MinMaxScaler()
robust = RobustScaler()
scaler = StandardScaler()

#Dropping categorical features
data_scaled = data.drop(columns=['BMI_status','Pressure_status','Diabetes_status','test_result'],axis=1)

#after dropping
data_scaled.head()

#Scalling all the numerical features

data_scaled['preg_scaled'] = robust.fit_transform(data_scaled[['Pregnancies']])
data_scaled['chance_scaled'] = robust.fit_transform(data_scaled[['Diabetes_Chance']])
data_scaled['age_scaled'] = robust.fit_transform(data_scaled[['Age']])
data_scaled['skin_scaled'] = robust.fit_transform(data_scaled[['SkinThickness']])
data_scaled['insulin_scaled'] = robust.fit_transform(data_scaled[['Insulin']])
data_scaled['glucose_scaled'] = robust.fit_transform(data_scaled[['Glucose']])
data_scaled['blood_scaled'] = robust.fit_transform(data_scaled[['BloodPressure']])
data_scaled['bmi_scaled'] = robust.fit_transform(data_scaled[['BMI']])

#dropping the features after scalling
data_scaled.drop(columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','Diabetes_Chance',
                         'Age','Outcome'],axis=1,inplace=True)

#just moving the target to the back
data_scaled['Outcome']=data['Outcome']

data_scaled.head()

"""It is neat, but we need to transform the is_obese and is_diabetic feature into numerical variable using OneHotEncoder because they are nominal data type"""

#Importing OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

#Defining the encoder
enc = OneHotEncoder(handle_unknown='ignore')

col = sorted(data['is_obese'].unique().tolist()) + sorted(data['is_diabetic'].unique().tolist())

#Encoding the features
enc_df = pd.DataFrame(enc.fit_transform(data_scaled[['is_obese', 'is_diabetic']]).toarray(), columns=col)

enc_df.head()

#Joining the scaled dataframe with encoded dataframe
data_scaled = data_scaled.join(enc_df)
data_scaled.head()

#creating dummies
index_data = pd.get_dummies(data_scaled[['is_obese', 'is_diabetic']], drop_first=True)

index_data.head()

# joining dummies to the dataframe
final_data=data_scaled.join(index_data)
final_data.head()

#dropping unused features
final_data.drop(columns=['is_obese','is_diabetic','No','Yes'],axis=1,inplace=True)

final_data.head()

"""# 8. Balancing Data
Balancing data akan menggunakan teknik oversampling agar tidak ada data yang terbuang. Metode yang digunakan adalah SMOTE agar variasi dari data juga bertambah
"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from collections import Counter

#separating independent from dependent variables
X = final_data[['glucose_scaled','insulin_scaled','bmi_scaled','chance_scaled','age_scaled','is_obese_Yes']] 
y = final_data['Outcome']

X.head()

#split train-test data
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)

X_train.shape

y_train.shape

y_train.value_counts()

print('Before Oversampling: ',Counter(y_train))
#defining smote
SMOTE = SMOTE(random_state=0)

#fit and apply the transform
X_train_smote, y_train_smote = SMOTE.fit_resample(X_train,y_train)

#summarize class distribution
print('After Oversampling: ',Counter(y_train_smote))

"""# 9. Machine Learning Modeling
We don't use regression, kita pakai classification model
"""

# Importing packages
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

"""## Decision Tree"""

# Decision Tree without resampling
dt = DecisionTreeClassifier(random_state=0)
dt.fit(X_train, y_train)

# Decision Tree with resampling
dt2 = DecisionTreeClassifier(random_state=0)
dt2.fit(X_train_smote, y_train_smote)

"""## KNN"""

# KNN without resampling
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# KNN with resampling
knn2 = KNeighborsClassifier()
knn2.fit(X_train_smote, y_train_smote)

"""## SVC Linear"""

from sklearn.svm import SVC

# SVCL without resampling
svc = SVC(kernel='linear',random_state=0)
svc.fit(X_train,y_train)

# SVCL with resampling
svc2 = SVC(kernel='linear',random_state=0)
svc2.fit(X_train_smote,y_train_smote)

"""## SVC RBF"""

# SVCR without resampling
svc3 = SVC(kernel='rbf',random_state=0)
svc3.fit(X_train,y_train)

# SVCR with resampling
svc4 = SVC(kernel='rbf',random_state=0)
svc4.fit(X_train_smote,y_train_smote)

"""## RANDOM FOREST"""

# Random Forest without resampling
rf1 = RandomForestClassifier(random_state=0)
rf1.fit(X_train,y_train)

# Random Forest with resampling
rf2 = RandomForestClassifier(random_state=0)
rf2.fit(X_train_smote,y_train_smote)

"""## Naive Bayes"""

# Naive Bayes without resampling
nb1 = GaussianNB()
nb1.fit(X_train,y_train)

# Naive Bayes with resampling
nb2 = GaussianNB()
nb2.fit(X_train_smote,y_train_smote)

"""## Logistic Regression"""

# Naive Bayes without resampling
lr1 = LogisticRegression()
lr1.fit(X_train,y_train)

# Naive Bayes with resampling
lr2 = LogisticRegression()
lr2.fit(X_train_smote,y_train_smote)

"""# EVALUATION"""

from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, silhouette_score
from sklearn.metrics import classification_report

"""## Decision Tree Classifier

### DT no Resampling
"""

y_pred = dt.predict(X_test)
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_dt = dt.predict(X_test)
print(classification_report(y_test, y_pred_dt))

"""### DT with Resampling"""

y_pred_smote = dt2.predict(X_test)
acc = accuracy_score(y_test, y_pred_smote)
prec = precision_score(y_test, y_pred_smote)
recall = recall_score(y_test, y_pred_smote)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_dt2 = dt2.predict(X_test)
print(classification_report(y_test, y_pred_dt2))

"""## KNN

### KNN No Resampling
"""

y_pred_knn = knn.predict(X_test)
acc = accuracy_score(y_test, y_pred_knn)
prec = precision_score(y_test, y_pred_knn)
recall = recall_score(y_test, y_pred_knn)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_knn = knn.predict(X_test)
print(classification_report(y_test, y_pred_knn))

"""### KNN With Resampling"""

y_pred_knn2 = knn2.predict(X_test)
acc = accuracy_score(y_test, y_pred_knn2)
prec = precision_score(y_test, y_pred_knn2)
recall = recall_score(y_test, y_pred_knn2)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_knn2 = knn2.predict(X_test)
print(classification_report(y_test, y_pred_knn2))

"""## SVC Linear

### SVCL No Resampling
"""

y_pred_svc = svc.predict(X_test)
acc = accuracy_score(y_test, y_pred_svc)
prec = precision_score(y_test, y_pred_svc)
recall = recall_score(y_test, y_pred_svc)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_svc = svc.predict(X_test)
print(classification_report(y_test, y_pred_svc))

"""### SVCL With Resampling"""

y_pred_svc2 = svc2.predict(X_test)
acc = accuracy_score(y_test, y_pred_svc2)
prec = precision_score(y_test, y_pred_svc2)
recall = recall_score(y_test, y_pred_svc2)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_svc2 = svc2.predict(X_test)
print(classification_report(y_test, y_pred_svc2))

"""## SVC RBF

### SVCR No Resampling
"""

y_pred_svc3 = svc3.predict(X_test)
acc = accuracy_score(y_test, y_pred_svc3)
prec = precision_score(y_test, y_pred_svc3)
recall = recall_score(y_test, y_pred_svc3)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_svc3 = svc3.predict(X_test)
print(classification_report(y_test, y_pred_svc3))

"""### SVCR With Resampling"""

y_pred_svc4 = svc4.predict(X_test)
acc = accuracy_score(y_test, y_pred_svc4)
prec = precision_score(y_test, y_pred_svc4)
recall = recall_score(y_test, y_pred_svc4)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_svc4 = svc4.predict(X_test)
print(classification_report(y_test, y_pred_svc4))

"""## RANDOM FOREST

### RANDOM FOREST NO RESAMPLING
"""

y_pred_rf1 = rf1.predict(X_test)
acc = accuracy_score(y_test, y_pred_rf1)
prec = precision_score(y_test, y_pred_rf1)
recall = recall_score(y_test, y_pred_rf1)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_rf1 = rf1.predict(X_test)
print(classification_report(y_test, y_pred_rf1))

"""### RANDOM FOREST WITH RESAMPLING"""

y_pred_rf2 = rf2.predict(X_test)
acc = accuracy_score(y_test, y_pred_rf2)
prec = precision_score(y_test, y_pred_rf2)
recall = recall_score(y_test, y_pred_rf2)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_rf2 = rf2.predict(X_test)
print(classification_report(y_test, y_pred_rf2))

"""## Naive Bayes

Naive Bayes NO RESAMPLING

X = final_data[['glucose_scaled','insulin_scaled','bmi_scaled','chance_scaled','age_scaled','is_obese_Yes']] 
M
"""

y_pred_nb1 = nb1.predict(X_test)
acc = accuracy_score(y_test, y_pred_nb1)
prec = precision_score(y_test, y_pred_nb1)
recall = recall_score(y_test, y_pred_nb1)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_nb1 = nb1.predict(X_test)
print(classification_report(y_test, y_pred_nb1))

"""### Naive Bayes WITH RESAMPLING"""

y_pred_nb2 = nb2.predict(X_test)
acc = accuracy_score(y_test, y_pred_nb2)
prec = precision_score(y_test, y_pred_nb2)
recall = recall_score(y_test, y_pred_nb2)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_nb2 = nb2.predict(X_test)
print(classification_report(y_test, y_pred_nb2))

"""### Logistic Regression NO RESAMPLING"""

y_pred_lr1 = lr1.predict(X_test)
acc = accuracy_score(y_test, y_pred_lr1)
prec = precision_score(y_test, y_pred_lr1)
recall = recall_score(y_test, y_pred_lr1)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_lr1 = lr1.predict(X_test)
print(classification_report(y_test, y_pred_lr1))

"""### Logistic Regression WITH RESAMPLING"""

y_pred_lr2 = lr2.predict(X_test)
acc = accuracy_score(y_test, y_pred_lr2)
prec = precision_score(y_test, y_pred_lr2)
recall = recall_score(y_test, y_pred_lr2)

pred = (acc*100, prec*100, recall*100)
pred

y_pred_lr2 = lr2.predict(X_test)
print(classification_report(y_test, y_pred_lr2))

"""# Hyperparameter Tuning

**RANDOMIZED SEARCH**
"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedShuffleSplit


param_grid = { 'C' : [0.1, 1. ,  1000, 1000000],
              'gamma' : [ 0.001, 0.0001, 0.00001], 
              'kernel' : ['rbf']}

# Define random search
random_search = RandomizedSearchCV(estimator= SVC(), 
                           param_distributions=param_grid)

random_search.fit(X_train_smote,y_train_smote)

random_search.best_params_

best_random=random_search.best_estimator_

y_pred_random_svc = best_random.predict(X_test)
print(classification_report(y_test, y_pred_random_svc))

"""**GRID SEARCH**"""

from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
import numpy as np
import sklearn.metrics as metrics
from sklearn.metrics import f1_score, fbeta_score, make_scorer

# "C": [00.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000],
#"gamma": [1.e-09,1.e-08, 1.e-07, 1.e-06, 1.e-05, 0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]



#create a grid of parameters

param_grid={     'C' : [0.1, 1. ,  1000, 1000000],
              'gamma' : [ 0.001, 0.0001, 0.00001]
             }

classifier=SVC()

grid_search=GridSearchCV(estimator=classifier, param_grid=param_grid, 
                  cv=5)

# Define grid search
grid_search.fit(X_train_smote,y_train_smote)

grid_search.best_params_

best_grid=grid_search.best_estimator_

y_pred_grid_svc = best_grid.predict(X_test)
print(classification_report(y_test, y_pred_grid_svc))